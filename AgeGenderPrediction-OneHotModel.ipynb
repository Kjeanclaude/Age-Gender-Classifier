{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age and Gender Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Introduction\"></a>\n",
    "[INTRODUCTION](#Introduction)\n",
    "\n",
    "The goal of this project is to create an **Age-Gender Predictor**. It was inspired by the [Age and Gender Classification Dataset of the Open University of Israel (Face Image Project)](http://www.openu.ac.il/home/hassner/projects/cnn_agegender/CNN_AgeGenderEstimation.pdf).\n",
    "The intended goal of the project is to generate age gender on unseen face images.\n",
    "\n",
    "The \"Age and gender classification using convolutional neural networks\" is a research paper related to the dataset above providing separately age or gender labels.\n",
    "\n",
    "\n",
    "The main challenges are:\n",
    "- **1- Find an age and gender labeled dataset to train our implementations :** Find dozen of face images from the internet with their correct age and gender is easy. But when you talk about several thousands of images, ***the task is very hard and expensive***. So, during our research we coincidently found the free ***Age and Gender Classification Dataset of the Open University of Israel (Face Image Project)*** which as 8 classes of age instead of 6 as in the original paper. ***A lot of thanks to them.***\n",
    "- **2- Transform the dataset to reach our goals :** The Age and Gender Classification Dataset has some very good implementations in Tensorflow for Data preparation and tests. But they have a different goal, their implementation only performs seperate prediction of genders and ages. The goal here is to produce both prediction on each face image and try to outperform the accuracy. So, we should rewrite our own functions to reach our goals, using their annotations for the provided dataset. We need to match separate age and gender with their corresponding image ID and implement the prediction model.\n",
    "- **3- Try our implementation on video and mobile app (Optional) :** It is not too hard to label pictures in video when we don't have to simultaneously deal with the audio too. But we could have some surprises sometime because of frameworks dependencies. Moreover, at this time (April 2017) we don't have enough ressources about the transformation process of Deep learning Frameworks projects into mobile apps. Run a Deep learning code in a notebook implementation and run it as mobile app are very different. Each framework has its constraints, no universal methods for this (crossplatform iOS-Android-Windows Mobile, several frameworks, etc.). \n",
    "\n",
    "The main steps of implementation are:\n",
    "- **1- Data preparation :** create a dictionary of age and gender including all training set images\n",
    "- **2- One-hot labels :** create a dictionary of age and gender including all training set images\n",
    "- **3- Age and Gender Prediction :** create an ANN and train it with the prepared data to predict age and gender.\n",
    "- **4- Mobile app implementation :** \n",
    "\n",
    "## Requirements\n",
    "In the **import section** you will find all the required frameworks for the code to work. We mainly work with :\n",
    "\n",
    "- **Tensorflow** and its advanced API **Tflearn**.\n",
    "- **Python 2.x** and **Python 3.x** branches (You could run the notebook code where you want)\n",
    "\n",
    "\n",
    "Below the Table of contents. We try to be as clear as possible for pedagogical reasons, so that newbies could also get what they need.\n",
    "<a name=\"Table_of_Contents\"></a>\n",
    "# Table of Contents\n",
    "- [Introduction](#Introduction)\n",
    "\n",
    "- [I- Data Preparation](#Data_Preparation)\n",
    "    - [I.1- Fondamental functions for data preparation](#Fondamental_functions)\n",
    "    - [I.2- The One-Hot Labels](#One_Hot_Labels)\n",
    "      - [I.2.1- The One-Hot Labels definition](#One_Hot_Labels_definition)\n",
    "      - [I.2.2- The One-Hot Labels creation for all features](#One_Hot_Labels_creation)\n",
    "    - [I.3- Datasets Transformation into lists](#Datasets_Transformation)\n",
    "    - [I.4- Training and Test sets creation](#Training_Test_Sets_Creation)\n",
    "\n",
    "\n",
    "- [II- Age and Gender Prediction](#Age_Gender_Prediction)\n",
    "    - [II.1- Build the Artificial Neural Network (ANN) Model](#ANN_Model)\n",
    "      - [II.1.1- With TFLearn](#ANN_Model_TFlearn)\n",
    "      - [II.1.2- With Tensorflow](#ANN_Model_Tensorflow)\n",
    "\n",
    "    - [II.2- Models Evaluation](#Model_Evaluation)\n",
    "      - [II.2.1- With TFLearn](#Model_Evaluation_TFlearn)\n",
    "      - [II.2.2- With Tensorflow](#Model_Evaluation_Tensorflow)\n",
    "\n",
    "    - [II.3- Prediction on the Models](#Model_Prediction)\n",
    "      - [II.3.1- With TFLearn](#Model_Prediction_TFlearn)\n",
    "        - [II.3.1.1- With images from the dataset](#prediction_on_dataset_TFlearn)\n",
    "        - [II.3.1.2- With other face images](#prediction_on_otherFaces_TFlearn)\n",
    "        - [II.3.1.3- With non-faces images](#prediction_on_NonFaces_TFlearn)\n",
    "      - [II.3.2- With Tensorflow](#Model_Prediction_Tensorflow)\n",
    "        - [II.3.2.1- With images from the dataset](#prediction_on_dataset_Tensorflow)\n",
    "        - [II.3.2.2- With other face images](#prediction_on_otherFaces_Tensorflow)\n",
    "        - [II.3.2.3- With non-faces images](#prediction_on_NonFaces_Tensorflow)\n",
    "\n",
    "    - [II.4- Improvement](#Improvement)\n",
    "      - [II.4.1- With TFLearn](#Improvement_TFlearn)\n",
    "          - (Hyperparameter tuning and prediction)\n",
    "      - [II.4.2- With Tensorflow](#Improvement_Tensorflow)\n",
    "          - (Hyperparameter tuning and prediction)\n",
    " \n",
    " \n",
    "- [III- Age and Gender Application (Optional)](#Age_Gender_Application)\n",
    "    - Resources for Desktop, Mobile and Web Application     \n",
    "    \n",
    "    \n",
    "- [IV- Discussion](#Discussion)\n",
    "    - Improvement\n",
    "    - Framework limitation\n",
    "    - Future work\n",
    "\n",
    "\n",
    "- [CONCLUSION](#Conclusion)\n",
    "    - Tools of the MLND used in the capstone Age and Gender Prediction implementation.\n",
    "\n",
    "\n",
    "- [VI- References](#References)\n",
    "      - https://www.tensorflow.org\n",
    "      - http://tflearn.org\n",
    "      - https://keras.io\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the required libraries\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from scipy.misc import imresize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"tf.__version__ : \", tf.__version__)\n",
    "print(\"python --version : \", sys.version)\n",
    "#print(\"tflearn --version : \", tflearn.__version__)\n",
    "PyVersion = sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Data_Preparation\"></a>\n",
    "# I- Data preparation\n",
    "<a name=\"Fondamental_functions\"></a>\n",
    "## I.1- Fondamental functions for data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the differents training set files \n",
    "\n",
    "The goal is to create all the necessary functions to link each image to its gender and age, such as *age_gender_per_image*).\n",
    "    \n",
    "    To do it we will use the files in [./AgeGenderDeepLearning/Folds/train_val_txt_files_per_fold] directory. In each fold we have these files : {age_test.txt, age_train.txt, age_train_subset.txt, age_val.txt, gender_test.txt, gender_train.txt, gender_train_subset.txt, gender_val.txt}.\n",
    "* The functions will mainly keep images ids in dictionaries of age and gender. For example : ***age_gender_dic = {\"id_image\":(age_class, gender)}***\n",
    "\n",
    "    <font color='red'>Note: Here we do not work on the dataset itself directly. But we only use the images' names saved in the files.<br\\>For the ***images_root_directory = '/notebooks/faces'*** test purposes below, consider that ***/notebooks/*** is the ipython notebook repository when runing it on your own system; and update if necessary.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE FOLDS PATH : TRAINING, VALIDATION AND TEST FOLDS\n",
    "print(\"PyVersion[0] : '{}'\".format(PyVersion[0]))\n",
    "\n",
    "if int(PyVersion[0])==2:\n",
    "    folds_path='Folds/train_val_txt_files_per_fold' \n",
    "    print(\"Jean-Claude, I got in python 2\")\n",
    "else:\n",
    "    folds_path='Folds\\\\train_val_txt_files_per_fold'\n",
    "    print(\"Jean-Claude, I got in python 3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TESTED WITH SUCCESS ON THE 1Go faces Dataset\n",
    "\n",
    "\n",
    "def is_image_in_gender_dic(image_id):\n",
    "    # Verify if an image is in gender_dictionary and return true or false\n",
    "    # Just to make the age_gender_dic function aesthetic\n",
    "    if image_id in gender_dic:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def is_image_in_age_dic(image_id):\n",
    "    # Verify if an image is in age_dictionary and return true or false\n",
    "    # Just to make the age_gender_dic function aesthetic\n",
    "    if image_id in age_dic:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def python3_image_id(image_id):\n",
    "    # Let's get the image_id with python 3\n",
    "    image_ids = image_id.split('/')\n",
    "    i=1\n",
    "    image_id2 = ''\n",
    "    for elt in image_ids:\n",
    "        if i<len(image_ids):\n",
    "            image_id2 += elt+'\\\\'\n",
    "            i=i+1\n",
    "        else:\n",
    "            image_id2 += elt\n",
    "            i=i+1\n",
    "            \n",
    "    return image_id2\n",
    "\n",
    "def get_image_from_directory(images_root_directory, image_id):\n",
    "    # The goal of this function is to return an image with its full path name.\n",
    "    # Useful for transformation process.\n",
    "    \n",
    "    # Get the image directory from its id\n",
    "    if int(PyVersion[0])==2:\n",
    "        image_directory = image_id.split('/')[0]\n",
    "        image_directory=os.path.join(images_root_directory,image_directory)\n",
    "        image_directory_files_names = os.listdir(image_directory)\n",
    "    else:\n",
    "        image_directory = image_id.split('\\\\')[0]\n",
    "        image_directory=os.path.join(images_root_directory,image_directory)\n",
    "        image_directory_files_names = os.listdir(image_directory)\n",
    "    #print(\"image_directory in get_image_from_directory :\\n {} \\n\".format(image_directory))\n",
    "\n",
    "    #print(\"image_directory_files_names in get_image_from_directory :\\n {} \\n\".format(image_directory_files_names))\n",
    "    \n",
    "    # Get the image itself in a variable with its full path,\n",
    "    # so that we could directly apply any image function on\n",
    "    # We save the part of the image name which is unique, in image_id_in_fold \n",
    "    image_id_in_fold = image_id.split('.')[1]\n",
    "    image_id_in_fold = image_id_in_fold + '.' + image_id.split('.')[2]\n",
    "    #print(\"image_id_in_fold in get_image_from_directory :\\n {} \\n\".format(image_id_in_fold))\n",
    "    \n",
    "    # We only select images files (.jpg), according to the Adience benchmark\n",
    "    image_name = [image_file for image_file in image_directory_files_names if image_id_in_fold in image_file \n",
    "                  and '.jpg' in image_file]\n",
    "\n",
    "    \n",
    "    # Get now the image full path name\n",
    "    image = os.path.join(image_directory,image_name[0])\n",
    "    \n",
    "    return image\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def age_gender_per_image(folds_path, images_root_directory):\n",
    "    # Extract all files content in a list.\n",
    "    #folds_path='/notebooks/AgeGenderDeepLearning/Folds/train_val_txt_files_per_fold'\n",
    "    files_root_dir=folds_path # txt files directory\n",
    "    all_images_list = [] \n",
    "    gender = [] \n",
    "    age = []\n",
    "    #current_test_files_names = []\n",
    "    age_gender_dic = {}\n",
    "    gender_dic = {}\n",
    "    age_dic = {}\n",
    "    \n",
    "    \n",
    "    # Loop all the folds in the main directory\n",
    "    # This will give two lists of images : gender (each image with its gender) and age (each image with its age)\n",
    "    for current_test_fold_ind in range(5):\n",
    "        current_fold_name='test_fold_is_{0}'.format(current_test_fold_ind)\n",
    "        current_fold_name=os.path.join(files_root_dir,current_fold_name)\n",
    "        #print(\"TEST current_fold_name : \", current_fold_name)\n",
    "        \n",
    "        \n",
    "        # Read and save the test files names in a variable\n",
    "        \n",
    "        #current_test_files_names = os.listdir(current_fold_name)\n",
    "        if int(PyVersion[0])==2:\n",
    "            current_test_files_names = os.listdir(current_fold_name)\n",
    "            #print(\"TEST current_test_files_names : \", current_test_files_names)\n",
    "        elif int(PyVersion[0])==3:\n",
    "            current_test_files_names = os.listdir(os.path.abspath(current_fold_name))\n",
    "            #print(\"TEST current_test_files_names : \", current_test_files_names)\n",
    "            \n",
    "        # Loop over the files names from the variable\n",
    "        # and for each file, read it and save the content in a variable of images names\n",
    "        # Save the gender files content in [gender] and the age files content in [age]\n",
    "        for file_name in current_test_files_names:\n",
    "            current_file_txt = os.path.join(current_fold_name,file_name)\n",
    "            # Read the current_file_txt content and save it according to its type (gender or age file)\n",
    "            \n",
    "            # If it is a age file txt \n",
    "            if 'age' in file_name:\n",
    "                # Read the current_file_txt content\n",
    "                with open(current_file_txt) as f:\n",
    "                    current_file_txt_content=f.readlines()\n",
    "                    \n",
    "                # Save the content in age list    \n",
    "                for elt in current_file_txt_content:\n",
    "                    elt=elt.split('\\r\\n')[0]\n",
    "                    age.append(elt)\n",
    "                    image_name=elt.split(' ')[0]\n",
    "                    age_class=elt.split(' ')[1]\n",
    "                    age_dic[image_name]=age_class\n",
    "                    \n",
    "                    # Add the image in the list of all images\n",
    "                    if image_name not in all_images_list:\n",
    "                        all_images_list.append(image_name)\n",
    "                    \n",
    "            \n",
    "            # If it is a gender file txt    \n",
    "            if 'gender' in file_name:\n",
    "                # Read the current_file_txt content\n",
    "                with open(current_file_txt) as f:\n",
    "                    current_file_txt_content=f.readlines()\n",
    "                    \n",
    "                # Save the content in age list    \n",
    "                for elt in current_file_txt_content:\n",
    "                    elt=elt.split('\\r\\n')[0]\n",
    "                    gender.append(elt)\n",
    "                    image_name=elt.split(' ')[0]\n",
    "                    gender_digit=elt.split(' ')[1]\n",
    "                    gender_dic[image_name]=gender_digit\n",
    "                    \n",
    "                    # Add the image in the list of all images\n",
    "                    if image_name not in all_images_list:\n",
    "                        all_images_list.append(image_name)\n",
    "    \n",
    "    \n",
    "    ### NOW age and gender dictionaries are filled and all images are saved in one dictionary\n",
    "    # So we are ready to create the age_gender_dic\n",
    "    \n",
    "    # Fill the age_gender_dic\n",
    "    for image_id in all_images_list:\n",
    "        # Verify that image_id is in both\n",
    "        if image_id in gender_dic and image_id in age_dic:\n",
    "            # Add it to the age_gender_dic\n",
    "            if int(PyVersion[0])==2:\n",
    "                image_full_path_name = get_image_from_directory(images_root_directory, image_id)\n",
    "                age_gender_dic[image_full_path_name] = (int(age_dic[image_id]), int(gender_dic[image_id]))\n",
    "            elif int(PyVersion[0])==3:\n",
    "                image_full_path_name = get_image_from_directory(images_root_directory, image_id)\n",
    "                age_gender_dic[image_full_path_name] = (int(age_dic[image_id]), int(gender_dic[image_id]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Test with the get_image_from_directory  aligned\n",
    "    ### FOR TEST PURPOSES ONLY, if you don't want to display, put it in comment.\n",
    "    if int(PyVersion[0])==2:\n",
    "        image_id = '28754132@N06/landmark_aligned_face.610.11612814564_0fa84cd8bb_o.jpg'\n",
    "        image_full_path_name = get_image_from_directory(images_root_directory, image_id)\n",
    "        print(\"(sample of) image_full_path_name :\\n{} \\n\\n\".format(image_full_path_name))\n",
    "    elif int(PyVersion[0])==3:\n",
    "        image_id = '28754132@N06\\\\landmark_aligned_face.610.11612814564_0fa84cd8bb_o.jpg'\n",
    "        image_full_path_name = get_image_from_directory(images_root_directory, image_id)\n",
    "        print(\"(sample of) image_full_path_name :\\n{} \\n\\n\".format(image_full_path_name))\n",
    "    \n",
    "    \n",
    "    return age_gender_dic\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n ************ Test of the age_gender_dic function **************  \")\n",
    "print(\"\\n ******** ALL STATISTICS ABOUT THE DATA AVAILABLE HERE *********  \\n\")\n",
    "# Test of the function\n",
    "age_gender_dic = {}\n",
    "\n",
    "\n",
    "if int(PyVersion[0])==2:\n",
    "    images_root_directory = 'faces' # Uncomment and comment the line below to work only with /faces dataset\n",
    "    #images_root_directory = '/notebooks/aligned' # Uncomment to work only with /aligned dataset\n",
    "elif int(PyVersion[0])==3:  \n",
    "    images_root_directory = 'faces' \n",
    "    images_root_directory = 'aligned'   \n",
    "\n",
    "\n",
    "age_gender_dic = age_gender_per_image(folds_path, images_root_directory)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n ************ ********* **************  \\n\")\n",
    "    \n",
    "print(\"age_gender_dic len : {}\".format(len(age_gender_dic)))\n",
    "\n",
    "i = 0\n",
    "for elt in age_gender_dic:\n",
    "    if i==0:\n",
    "        print(\"age_gender_dic image id : {} \\nage_gender_dic classification : {}\".format(elt, age_gender_dic[elt]))\n",
    "        #print(\"age_gender_dic len (sample): \\n\", age_gender_dic[elt])\n",
    "        i=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(age_gender_dic.values()))\n",
    "age_gender_dic.values()[:12] # A sample of the values list content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ******** Detailed Information about the Unified Dataset Benchmark *********  \\n\")\n",
    "\"\"\"\n",
    "Write a script on the Unified Dataset which return\n",
    "for each age class:\n",
    "- the number of male\n",
    "- the number of female\n",
    "- the total\n",
    "\"\"\"\n",
    "content_dic = {\"Male\":0, \"Female\":0} # Will totalize the content of the Unified_Dataset per class\n",
    "Unified_Dataset = {\"0-2\":content_dic, \"4-6\":content_dic, \"8-13\":content_dic, \"15-20\":content_dic, \n",
    "                   \"25-32\":content_dic, \"38-43\":content_dic, \"48-53\":content_dic, \"60-\":content_dic}\n",
    "\n",
    "def return_age_class_key(id):\n",
    "    age_class_key_dic = {0:\"0-2\", 1:\"4-6\", 2:\"8-13\", 3:\"15-20\", 4:\"25-32\", 5:\"38-43\", 6:\"48-53\", 7:\"60-\"}\n",
    "    return age_class_key_dic[id]\n",
    "\n",
    "# Let's test the function\n",
    "print(\"return_age_class_key test :\", return_age_class_key(7))\n",
    "\n",
    "# In the Adience Benchmark txt files, male are label with a 0, and female with a 1.\n",
    "def return_Unified_Dataset_Details(age_gender_dic, Unified_Dataset):\n",
    "    for value in age_gender_dic.values():\n",
    "        age_class, gender_class = value\n",
    "        current_class = return_age_class_key(int(age_class))\n",
    "        #print(\"I got value JC : \", value)\n",
    "        #print(\"current_class JC : \", current_class)\n",
    "        if int(gender_class) == 0:\n",
    "            current_content = Unified_Dataset[current_class]\n",
    "            current_content_dic = current_content.copy()\n",
    "            current_male_value = current_content_dic[\"Male\"]\n",
    "            #print(\"current_male_value : \", current_male_value)\n",
    "            current_content_dic[\"Male\"] = int(current_male_value) + 1\n",
    "            #print(\"current_male_value After Update : \", current_male_value)\n",
    "            Unified_Dataset[current_class] = current_content_dic\n",
    "            current_content_dic = {}\n",
    "        elif int(gender_class) == 1:\n",
    "            current_content = Unified_Dataset[current_class]\n",
    "            current_content_dic = current_content.copy()\n",
    "            current_female_value = current_content_dic[\"Female\"]\n",
    "            current_content_dic[\"Female\"] = int(current_female_value) + 1\n",
    "            Unified_Dataset[current_class] = current_content_dic\n",
    "            current_content_dic = {}\n",
    "            \n",
    "    return Unified_Dataset\n",
    "            \n",
    "            \n",
    "Unified_Dataset_result = return_Unified_Dataset_Details(age_gender_dic, Unified_Dataset)\n",
    "\n",
    "print(\"\\nUnified Dataset Details :\\n\", Unified_Dataset_result)\n",
    "\n",
    "total_number=0\n",
    "for each_class in Unified_Dataset_result:\n",
    "    content = Unified_Dataset_result[each_class]\n",
    "    total_number += int(content['Male'])\n",
    "    total_number += int(content['Female'])\n",
    "\n",
    "print(\"\\nTotal number of images : \", total_number)\n",
    "\n",
    "print(\"\\n ************ ********* **************  \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"One_Hot_Labels\"></a>\n",
    "## I.2- The One-Hot Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"One_Hot_Labels_definition\"></a>\n",
    "### I.2.1- The One-Hot Labels definition\n",
    "Here, we will see the One-Hot Label presentations and how it works. For pedagogical reason only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the age and gender function is OK, and return for each image its gender and age, we should use it during the training process. \n",
    "\n",
    "Now, we should define a conditonal placeholder (one-hot label) with a size of 10 (8 classes of age and 2 classes of gender). This will allow us to match each image with its label; a one-hot label with the age and gender digit set to 1. \n",
    "Here regarding the data structure we cannot use tools such us .... to automatically generate the one-hot label, instead we will hardcode its generation.\n",
    "Indeed, according to the selected labeled dataset we have two type of classes : \n",
    "\n",
    "    ---> age_list = ['(0, 2)','(4, 6)','(8, 12)','(15, 20)','(25, 32)','(38, 43)','(48, 53)','(60, 100)']\n",
    "    ---> gender_list = ['m','f']\n",
    "\n",
    "So the one-hot label should have the format below :\n",
    "\n",
    "    ---> one_hot_label_placeholder = (0,0,0,0,0,0,0,0,0,0)\n",
    "    ---> one_hot_label_placeholder = (ac1,ac2,ac3,ac4,ac5,ac6,ac7,ac8,gc1,gc2)\n",
    "    | ac1 = age class 1 | gc1 = gender class 1 |\n",
    "    \n",
    "For example for a boy teenager between 8 and 12 will have this one-hot labels : ***(0,0,1,0,0,0,0,0,1,0)***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"************** ONE-HOT LABEL USE CASES **************\\n\")\n",
    "\n",
    "# One-hot-labels_placeholder\n",
    "one_hot_label_placeholder = (0, 0, 0, 0, 0, 0, 0, 0, 0, 0) #OR\n",
    "one_hot_label_placeholder = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "y_dim = 10 # Number of classes\n",
    "y = tf.placeholder(tf.float32, shape=[None, y_dim])\n",
    "print(\"one_hot_label_placeholder with y : \", y)\n",
    "\n",
    "print(\"\\nOne_hot_label in 1 Dimension :\")\n",
    "y_sample1 = np.zeros(shape=[10])\n",
    "print(y_sample1)\n",
    "\n",
    "print(\"\\nOne_hot_label in 2 Dimensions :\")\n",
    "y_sample2 = np.zeros(shape=[5, y_dim])\n",
    "print(y_sample2)\n",
    "\n",
    "print(\"\\nAssign value to One_hot_label in 1 Dimension (y_sample1[8] = 1):\")\n",
    "y_sample1[8] = 1\n",
    "print(y_sample1)\n",
    "\n",
    "print(\"\\nAssign value to One_hot_label in 2 Dimensions (y_sample2[:,8] = 1):\")\n",
    "y_sample2[:,8] = 1\n",
    "print(y_sample2)\n",
    "\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"One_Hot_Labels_creation\"></a>\n",
    "### I.2.2- The One-Hot Labels creation for all features\n",
    "Here, using the ***age_gender_dic features*** we will create their corresponding targets or labels; ***the One-Hot Labels***.\n",
    "The age and gender labels have the one-hot-labels format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train y\n",
    "def One_Hot_Labels(age_gender_dic_features):\n",
    "    # Build the train_y dictionary from the train_x one.\n",
    "    One_Hot_Labels = {}\n",
    "    \n",
    "    y_dim = 10\n",
    "    y_label = np.zeros(shape=[y_dim])\n",
    "    #y_label[7] = 1\n",
    "    #print(y_label)\n",
    "    # gender_list = ['m','f']\n",
    "    i = 0\n",
    "    for image_id_with_path in age_gender_dic_features:\n",
    "        # Get the age and gender values from train_x\n",
    "        age, gender = age_gender_dic_features[image_id_with_path]\n",
    "        \n",
    "        ### FOR TEST PURPOSES ONLY, uncomment during the training process.\n",
    "        #if i<2:\n",
    "        #    print(\"age : {} | gender : {}\".format(age, gender))\n",
    "        #    i +=1\n",
    "        \n",
    "        \n",
    "        # Set the correct y_label according to these values \n",
    "        y_label[np.int_(age)] = 1\n",
    "        if np.int_(gender) == 0:\n",
    "            y_label[8] = 1\n",
    "        if np.int_(gender) == 1:\n",
    "            y_label[9] = 1\n",
    "        \n",
    "        # Add the created y_label to the train_y dic, keeping the image id\n",
    "        One_Hot_Labels[image_id_with_path] = y_label\n",
    "        \n",
    "        # reinitialize it for the future label\n",
    "        y_label = np.zeros(shape=[y_dim])\n",
    "    \n",
    "    return One_Hot_Labels\n",
    "\n",
    "print(\"\\n ************ One_Hot_Labels tests **************  \\n\")\n",
    "\n",
    "One_Hot_Labels = One_Hot_Labels(age_gender_dic)\n",
    "\n",
    "i = 0\n",
    "for elt in One_Hot_Labels:\n",
    "    if i<=3:\n",
    "        print(\"One_Hot_Labels id : {} | One_Hot_Labels y_label : {}\".format(elt, One_Hot_Labels[elt]))\n",
    "        i +=1\n",
    "print(\"\\n *****************************************  \\n\")\n",
    "\n",
    "print(\"One_Hot_Labels_dic len : {}\".format(len(One_Hot_Labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Datasets_Transformation\"></a>\n",
    "## I.3- Datasets Transformation into lists\n",
    "\n",
    "Until here we used dictionaries to facilitate the images and their corresponding labels manipulation. For future use cases, they need to be saved in lists, so that they could be fitted in the models we will use.\n",
    "Indeed, we will use the ***sklearn.model_selection.train_test_split*** tool to split the dataset into training and test sets. And its allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.\n",
    "\n",
    "- For each image in the ***age_gender_dic*** dictionary (its key), we will add it in the ***age_gender_feature_list***. \n",
    "     * The image key is the full path image name (a string), it should be encoded to be handled by the model. \n",
    "     We could do it :\n",
    "         + Using the **plt.imread(image_name_with_full_path)** function\n",
    "         + Using the **decode_jpeg** function from data.py (***[rude-carnie/data.py](https://github.com/dpressel/rude-carnie/blob/master/data.py)***) to decode the JPEG string as a 3-D float image Tensor, an RGB JPEG.\n",
    "         + Using the ***_convert_images*** function from cifar10.py ([Hvass-Labs/TensorFlow-Tutorials/cifar10.py](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/cifar10.py)) to convert the raw images from the data-files to floating-points.\n",
    "\n",
    "- For each image in the ***age_gender_dic*** dictionary (its key), we will add its value (***the corresponding label***) in the ***age_gender_label_list***.\n",
    "\n",
    "Doing so, the model will learn for each image in ***age_gender_feature_list*** its label (the corresponding one-hot label) in ***age_gender_label_list***. And it will be able to predict the labels on unseen images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IN CASE OF MEMORY ISSUE\n",
    "<font color='red'>Note: In case of memory issue slice the dataset with the **number_of_sample** variable.<br/>To work on the whole dataset, set this variable to a number greater than the dataset size.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR A PART OF THE DATA, IN CASE OF MEMORY ISSUE\n",
    "print(\"len age_gender_dic dic : {} | len One_Hot_Labels dic : {}\".format(len(age_gender_dic), len(One_Hot_Labels)))\n",
    "\n",
    "number_of_sample = 50000\n",
    "def age_gender_xy_lists(age_gender_dic, One_Hot_Labels):\n",
    "    # Build the train_y and train_x lists from their corresponding dictionaries.\n",
    "    age_gender_feature_list = []\n",
    "    age_gender_label_list = []\n",
    "    \n",
    "    # Before adding an image in the list, verify that it is in both train_x and train_y dictionaries\n",
    "    # This to ensure that during the training process each image will have its correct label\n",
    "    i=0\n",
    "    for image_full_path in age_gender_dic:\n",
    "        #print(\"I see the image full path JC ! : \", image)\n",
    "        # Append the numerical image format using plt.imread(image_name_with_full_path) \n",
    "        if image_full_path in One_Hot_Labels and i<number_of_sample: \n",
    "            image = plt.imread(image_full_path)\n",
    "\n",
    "            age_gender_feature_list.append(image)\n",
    "            age_gender_label_list.append(One_Hot_Labels[image_full_path])\n",
    "            i +=1\n",
    "    \n",
    "    # Then resize the square image to 227 x 227 pixels\n",
    "    age_gender_feature_list = [imresize(img_i, (227, 227)) for img_i in age_gender_feature_list]\n",
    "\n",
    "            \n",
    "    return age_gender_feature_list, age_gender_label_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n ************ age_gender_xy_lists tests **************  \\n\")\n",
    "age_gender_feature_list, age_gender_label_list =  age_gender_xy_lists(age_gender_dic, One_Hot_Labels)\n",
    "print(\"len age_gender_feature_list : {} | len age_gender_label_list : {}\".format(len(age_gender_feature_list), len(age_gender_label_list)))\n",
    "i=0\n",
    "for elt in age_gender_feature_list:\n",
    "    if i<=0:\n",
    "        print(\"age_gender_feature_list : {} | \\nage_gender_feature_list Image : {} | \\nage_gender_label_list y_label : {}\".format(age_gender_feature_list[i][1,:2], plt.imshow(age_gender_feature_list[i]), age_gender_label_list[i]))\n",
    "        print(\"age_gender_feature_list image Shape :\", elt.shape)\n",
    "        i +=1\n",
    "\n",
    "print(\"\\n *****************************************  \\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Training_Test_Sets_Creation\"></a>\n",
    "## I.4- Training and Test sets creation\n",
    "Now, we will use the ***sklearn.model_selection.train_test_split*** tool to split the dataset into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(age_gender_feature_list, age_gender_label_list, \n",
    "                                                    test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"len X_train: {}, len X_test: {}, len y_train: {}, len y_test: {}\".format(len(X_train), len(X_test), \n",
    "                                                                                 len(y_train), len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Age_Gender_Prediction\"></a>\n",
    "# II- Age and Gender Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model will learn for each image in ***age_gender_feature_list*** its label (the corresponding one-hot label) in ***age_gender_label_list***. And it will be able to predict the labels on unseen images.\n",
    "\n",
    "\n",
    "In this part, we will pass the time training an artificial neural network model to learn how to predict age and gender of unseen images. For this purposes, we will use the **X_train** and **y_train** we previously built. \n",
    "The model will learn for each image in ***X_train*** its label (the corresponding one-hot label) in ***y_train***. And so, it will be able to predict the labels on unseen images. It will generate a one-hot label of 10 dimensions. So, according to the value we will know the age and gender of the image.\n",
    "\n",
    "- This step will be useful for the mobile App. For each image the app should give its gender and age, and then propose the user to chose a class for aging process on the same image.\n",
    "\n",
    "<font color='red'><br/>Note: <br/>Here, remember that a model which works well could need **several days on a modern GPU to be trained**.<br/>So **it could take a long time to train a good model, moreover on CPU systems.**<br/>Because of this, we will save the training model for futur use.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ANN_Model\"></a>\n",
    "## II.1- Build the Artificial Neural Network (ANN) Model\n",
    "<a name=\"ANN_Model_TFlearn\"></a>\n",
    "### II.1.1- With TFLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "print(\"Current path: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ************ START Build Artificial neural network Model With TFLearn **************  \\n\")\n",
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, 227, 227, 3])\n",
    "\n",
    "net = tflearn.fully_connected(net, 64)\n",
    "net = tflearn.fully_connected(net, 32)\n",
    "net = tflearn.fully_connected(net, 32)\n",
    "net = tflearn.fully_connected(net, 16)\n",
    "net = tflearn.fully_connected(net, 16)\n",
    "net = tflearn.fully_connected(net, 10, activation='softmax')\n",
    "\n",
    "net = tflearn.regression(net, optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "tflearn_model = tflearn.DNN(net, tensorboard_dir='output/tflearn_logs', tensorboard_verbose=3)\n",
    "\n",
    "# Start training (apply gradient descent algorithm)\n",
    "print(\"len train_x : {} | len train_y : {}\".format(len(X_train), len(y_train)))\n",
    "tflearn_model.fit(X_train, y_train, n_epoch=1000, batch_size=128, show_metric=True)\n",
    "\n",
    "print(\"\\n ************ END Build Artificial neural network Model With TFLearn **************  \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXECUTE THIS TO SAVE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### MODEL BACKUP\n",
    "tflearn_model.save('output/model.tflearn.onehot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ANN_Model_Tensorflow\"></a>\n",
    "### II.1.2- With Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(X_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ************ START Build Artificial neural network Model With Tensorflow **************  \\n\")\n",
    "### Here we will use tensorflow keras model, it accepts list and numpy arrays as tflearn\n",
    "from tensorflow.contrib.keras.python.keras.layers.core import *\n",
    "from tensorflow.contrib.keras.python.keras.models import Sequential\n",
    "from tensorflow.contrib.keras.python.keras.models import Model\n",
    "\n",
    "\n",
    "# This returns a tensor\n",
    "inputs = tf.contrib.keras.layers.Input(shape=(154587,))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(64)(inputs)\n",
    "x = Dense(32)(x)\n",
    "x = Dense(32)(x)\n",
    "x = Dense(16)(x)\n",
    "x = Dense(16)(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "tf_model = Model(inputs=inputs, outputs=predictions)\n",
    "tf_model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Reshape it to map the content to the rest of the process\n",
    "X_train = np.array(X_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 154587))\n",
    "y_train = np.array(y_train)\n",
    "print(y_train.shape)\n",
    "y_train_new = y_train.copy()\n",
    "\n",
    "print(y_train_new.shape)\n",
    "history = tf_model.fit(X_train, y_train_new, epochs=20, validation_split=0.1, batch_size=16, verbose=0)  # starts training\n",
    "#history = model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, verbose=0)\n",
    "\n",
    "print(\"\\n ************ END Build Artificial neural network Model With Tensorflow **************  \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### MODEL BACKUP\n",
    "filepath = 'output/model.tensorflow.onehot'\n",
    "tf.contrib.keras.models.save_model(tf_model, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Model_Evaluation\"></a>\n",
    "## II.2- Models Evaluation\n",
    "The Metrics used for Models Evaluation here are **loss and accuracy on the test set**.\n",
    "We will also when possible print a visualization for a better understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Model_Evaluation_TFlearn\"></a>\n",
    "### II.2.1- With TFLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Run the command below in a terminal of the server where this notebook is executed \n",
    "### and see the result at http://192.168.99.100:6006/\n",
    "#cd tensorflow/tensorflow/examples/udacity/\n",
    "#tensorboard --logdir='tflearn_logs'\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "score = tflearn_model.evaluate(X_test, y_test, batch_size=128)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Model_Evaluation_Tensorflow\"></a>\n",
    "### II.2.2- With Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 154587))\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "score = tf_model.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "# http://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Model_Prediction\"></a>\n",
    "## II.3- Prediction on the Models\n",
    "<a name=\"Model_Prediction_TFlearn\"></a>\n",
    "### II.3.1- With TFLearn\n",
    "<a name=\"prediction_on_dataset_TFlearn\"></a>\n",
    "#### II.3.1.1- With images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n ************ START Model Prediction with TFLearn **************  \\n\")\n",
    "images_to_plot = []\n",
    "\n",
    "print(\"==============> WITH THE test dataset\")\n",
    "# Reshape for the plot function\n",
    "image_to_predict = (X_test[0])[np.newaxis]\n",
    "image_to_predict = np.reshape(image_to_predict, (227, 227, 3))\n",
    "\n",
    "images_to_plot.append(image_to_predict)\n",
    "\n",
    "# Reshape for the prediction model\n",
    "image_to_predict = (image_to_predict)[np.newaxis]\n",
    "print(\"test dataset image shape : \", image_to_predict.shape)\n",
    "print(\"Label Prediction on the test dataset image : \", tflearn_model.predict(image_to_predict))\n",
    "print(\"True Label Value : \", y_test[0])\n",
    "\n",
    "\n",
    "for elt in images_to_plot:\n",
    "    print(\"Images shapes for plot :\")\n",
    "    print(elt.shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"prediction_on_otherFaces_TFlearn\"></a>\n",
    "#### II.3.1.2- With other face images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==============> WITH my own image\")\n",
    "# Prediction with my own image\n",
    "\n",
    "if int(PyVersion[0])==2:\n",
    "    data_dir_jc_photo = 'Kouassi-Jean-Claude-ID.png'\n",
    "elif int(PyVersion[0])==3:\n",
    "    data_dir_jc_photo = 'Kouassi-Jean-Claude-ID.png'\n",
    "\n",
    "# Reshape for the plot function                \n",
    "jc_image = plt.imread(data_dir_jc_photo)\n",
    "jc_image = imresize(jc_image, (227, 227))\n",
    "images_to_plot.append(jc_image)\n",
    "\n",
    "# Reshape for the prediction model\n",
    "jc_image = jc_image[np.newaxis]\n",
    "jc_image = jc_image[:, :, :, :3]\n",
    "print(\"jc_image shape : \", jc_image.shape)\n",
    "print(\"Label Prediction with JC Photo: \", tflearn_model.predict(jc_image))\n",
    "print(\"True Label Value : [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"prediction_on_NonFaces_TFlearn\"></a>\n",
    "#### II.3.1.3- With non-faces images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==============> WITH a non-face image\")\n",
    "# Prediction on unsuitable image (several faces, or animals)\n",
    "if int(PyVersion[0])==2:\n",
    "    data_dir_other = 'Fish.JPEG'\n",
    "elif int(PyVersion[0])==3:\n",
    "    data_dir_other = 'Fish.JPEG'\n",
    "\n",
    "# Reshape for the plot function \n",
    "other_image = plt.imread(data_dir_other)\n",
    "other_image = imresize(other_image, (227, 227))\n",
    "images_to_plot.append(other_image)\n",
    "\n",
    "# Reshape for the prediction model\n",
    "other_image = other_image[np.newaxis]\n",
    "print(\"other_image shape : \", other_image.shape)\n",
    "print(\"Label Prediction with another image: \", tflearn_model.predict(other_image))\n",
    "print(\"True Label Value : Here we have a picture of fishes\")\n",
    "\n",
    "print(\"len images_to_plot :\", len(images_to_plot))\n",
    "\n",
    "for elt in images_to_plot:\n",
    "    print(\"Images shapes for plot :\")\n",
    "    print(elt.shape) \n",
    "\n",
    "fig, axs = plt.subplots(1, len(images_to_plot), figsize=(20, 4))\n",
    "for i, ax_i in enumerate(axs):\n",
    "    NewTitle = \"Test image number \", str(i+1)\n",
    "    ax_i.imshow(images_to_plot[i])\n",
    "    ax_i.set_title(NewTitle)\n",
    "    ax_i.grid('off')\n",
    "    ax_i.axis('off')\n",
    "    \n",
    "print(\"\\n ************ END Model Prediction with TFLearn**************  \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Model_Prediction_Tensorflow\"></a>\n",
    "### II.3.2- With Tensorflow \n",
    "\n",
    "<a name=\"prediction_on_dataset_Tensorflow\"></a>\n",
    "#### II.3.2.1- With images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n ************ START Model Prediction  with Tensorflow **************  \\n\")\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "Label = tf_model.predict(np.reshape(X_test[0], (1, 154587)))\n",
    "\n",
    "print(\"Label Prediction : \", Label)\n",
    "print(\"True Label Value : \", y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"prediction_on_otherFaces_Tensorflow\"></a>\n",
    "#### II.3.2.2- With other face images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before reshape : \", jc_image.shape)\n",
    "# So let's remove the Alpha channel on the blended image\n",
    "# https://github.com/davisking/dlib/issues/128\n",
    "jc_image = jc_image[:, :, :, :3]\n",
    "print(\"After reshape : \", jc_image.shape)\n",
    "score = tf_model.predict(np.reshape(jc_image, (1, 154587)))\n",
    "print(\"Prediction score : \", score)\n",
    "print(\"True Label Value : [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"prediction_on_NonFaces_Tensorflow\"></a>\n",
    "#### II.3.2.3- With non-faces images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(other_image.shape)\n",
    "\n",
    "score = tf_model.predict(np.reshape(other_image, (1, 154587)))\n",
    "print(\"Prediction score : \", score)\n",
    "print(\"True Label Value : Here we have a picture of fishes\")\n",
    "\n",
    "print(\"\\n\\n ************ END Model Prediction with Tensorflow **************  \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Improvement\"></a>\n",
    "## II.4- Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Improvement_TFlearn\"></a>\n",
    "### II.4.1- With TFLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The improved model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ************ START Build Artificial Improved neural network Model with TFLearn **************  \\n\")\n",
    "# https://github.com/tflearn/tflearn/blob/master/examples/basics/finetuning.py\n",
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, 227, 227, 3])\n",
    "\n",
    "\"\"\"\n",
    "# Building deep neural network\n",
    "# https://github.com/tflearn/tflearn/blob/master/examples/images/highway_dnn.py\n",
    "# A SAMPLE OF HIGHWAY FCN\n",
    "\n",
    "input_layer = net\n",
    "dense1 = tflearn.fully_connected(input_layer, 64, activation='relu',\n",
    "                                 regularizer='L2', weight_decay=0.001)\n",
    "                 \n",
    "                 \n",
    "#install a deep network of highway layers\n",
    "highway = dense1                              \n",
    "for i in range(10):\n",
    "    highway = tflearn.highway(highway, 64, activation='relu',\n",
    "                              regularizer='L2', weight_decay=0.001, transform_dropout=0.5)\n",
    "                              \n",
    "                              \n",
    "softmax = tflearn.fully_connected(highway, 10, activation='softmax')\n",
    "\n",
    "# Regression using SGD with learning rate decay and Top-3 accuracy\n",
    "# http://tflearn.org/metrics/\n",
    "sgd = tflearn.SGD(learning_rate=0.1, lr_decay=0.96, decay_step=1000)\n",
    "top_k = tflearn.metrics.Top_k(3)\n",
    "\n",
    "acc = tflearn.metrics.Accuracy ()\n",
    "net = tflearn.regression(softmax, optimizer=sgd, metric=acc,\n",
    "                         loss='categorical_crossentropy')\n",
    "\n",
    "# Training\n",
    "tflearn_improved_model = tflearn.DNN(net, tensorboard_dir='output/tflearn_logs', tensorboard_verbose=0)\n",
    "tflearn_improved_model.fit(X_train, y_train, n_epoch=120, validation_set=(X_test, y_test),\n",
    "          show_metric=True, run_id=\"highway_dense_model\")\n",
    "\"\"\"\n",
    "net = tflearn.fully_connected(net, 128, activation='linear')\n",
    "net = tflearn.fully_connected(net, 64, activation='linear')\n",
    "net = tflearn.fully_connected(net, 32, activation='linear')\n",
    "net = tflearn.fully_connected(net, 32, activation='linear')\n",
    "net = tflearn.fully_connected(net, 16, activation='linear')\n",
    "net = tflearn.fully_connected(net, 16, activation='linear')\n",
    "net = tflearn.fully_connected(net, 10, activation='softmax')\n",
    "\n",
    "\n",
    "with tf.name_scope('CustomMonitor'):\n",
    "    test_var = tf.reduce_sum(tf.cast(net, tf.float32), name=\"test_var\")\n",
    "    test_const = tf.constant(32.0, name=\"custom_constant\")\n",
    "\n",
    "# softmax_categorical_crossentropy\n",
    "# mean_square \n",
    "# hinge_loss \n",
    "net = tflearn.regression(net, optimizer='adam', loss='categorical_crossentropy', \n",
    "                         learning_rate=0.001, name='target', validation_monitors=[test_var, test_const])\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "tflearn_improved_model = tflearn.DNN(net, tensorboard_dir='output/tflearn_logs', tensorboard_verbose=3)\n",
    "\n",
    "# Start training (apply gradient descent algorithm)\n",
    "print(\"len train_x : {} | len train_y : {}\".format(len(X_train), len(y_train)))\n",
    "\n",
    "tflearn_improved_model.fit(X_train, y_train, n_epoch=3000, batch_size=128, show_metric=True, validation_set=0.2)\n",
    "\n",
    "print(\"\\n ************ END Build Artificial Improved neural network Model with TFLearn **************  \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# USEFUL INFORMATION FOR TUNING\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# tflearn.layers.core.dropout (incoming, keep_prob, noise_shape=None, name='Dropout')\n",
    "\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# Activation and Regularization inside a layer:\n",
    "# fc2 = tflearn.fully_connected(fc1, 32, activation='tanh', regularizer='L2')\n",
    "\n",
    "#tflearn.layers.core.fully_connected (incoming, n_units, activation='linear', bias=True, weights_init='truncated_normal', \n",
    "#                                     bias_init='zeros', regularizer=None, weight_decay=0.001, trainable=True, restore=True, \n",
    "#                                     reuse=False, scope=None, name='FullyConnected')\n",
    "\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# Fully Connected Highway\n",
    "# The dropout operation keeps the probabilities between layers\n",
    "#tflearn.layers.core.highway (incoming, n_units, activation='linear', transform_dropout=None, weights_init='truncated_normal', \n",
    "#                             bias_init='zeros', regularizer=None, weight_decay=0.001, trainable=True, restore=True, \n",
    "#                             reuse=False, scope=None, name='FullyConnectedHighway')\n",
    "\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#tflearn.layers.estimator.regression (incoming, placeholder='default', optimizer='adam', \n",
    "#                                     loss='categorical_crossentropy', metric='default', learning_rate=0.001, \n",
    "#                                     dtype=tf.float32, batch_size=64, shuffle_batches=True, to_one_hot=False, \n",
    "#                                     n_classes=None, trainable_vars=None, restore=True, op_name=None, \n",
    "#                                     validation_monitors=None, validation_batch_size=None, name=None)\n",
    "\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# Validation_monitors\n",
    "# In practice, the validation monitor may present useful information, like confusion matrix entries, or an AUC metric.\n",
    "#with tf.name_scope('CustomMonitor'):\n",
    "#    test_var = tf.reduce_sum(tf.cast(network, tf.float32), name=\"test_var\")\n",
    "#    test_const = tf.constant(32.0, name=\"custom_constant\")\n",
    "\n",
    "#network = regression(network, optimizer='adam', learning_rate=0.01,\n",
    "#                                 loss='categorical_crossentropy', name='target', validation_monitors=[test_var, test_const])\n",
    "\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# Training\n",
    "#fit (X_inputs, Y_targets, n_epoch=10, validation_set=None, show_metric=False, batch_size=None, shuffle=None, \n",
    "#     snapshot_epoch=True, snapshot_step=None, excl_trainops=None, validation_batch_size=None, run_id=None, callbacks=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL BACKUP\n",
    "tflearn_improved_model.save('output/improved.model.tflearn.onehot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Run the command below in a terminal of the server where this notebook is executed \n",
    "### and see the result at http://192.168.99.100:6006/\n",
    "#cd tensorflow/tensorflow/examples/udacity/\n",
    "#tensorboard --logdir='tflearn_logs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "score = tflearn_improved_model.evaluate(X_test, y_test, batch_size=128)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n ************ START Improved Model Prediction with TFLearn **************  \\n\")\n",
    "images_to_plot = []\n",
    "\n",
    "print(\"==============> WITH THE test dataset\")\n",
    "# Reshape for the plot function \n",
    "image_to_predict = (X_test[0])[np.newaxis]\n",
    "image_to_predict = np.reshape(image_to_predict, (227, 227, 3))\n",
    "images_to_plot.append(image_to_predict)\n",
    "\n",
    "# Reshape for the prediction model\n",
    "image_to_predict = (image_to_predict)[np.newaxis]\n",
    "print(\"test dataset image shape : \", image_to_predict.shape)\n",
    "print(\"Label Prediction on the test dataset image : \", tflearn_improved_model.predict(image_to_predict))\n",
    "print(\"True Label Value : \", y_test[0])\n",
    "\n",
    "for elt in images_to_plot:\n",
    "    print(\"Images shapes for plot :\")\n",
    "    print(elt.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n==============> WITH my own image\")\n",
    "# Prediction with my own image\n",
    "\n",
    "if int(PyVersion[0])==2:\n",
    "    data_dir_jc_photo = 'Kouassi-Jean-Claude-ID.png'\n",
    "elif int(PyVersion[0])==3:\n",
    "    data_dir_jc_photo = 'Kouassi-Jean-Claude-ID.png'\n",
    "\n",
    "# Reshape for the plot function \n",
    "jc_image = plt.imread(data_dir_jc_photo)\n",
    "jc_image = imresize(jc_image, (227, 227))\n",
    "images_to_plot.append(jc_image)\n",
    "\n",
    "# Reshape for the prediction model\n",
    "jc_image = jc_image[np.newaxis]\n",
    "jc_image = jc_image[:, :, :, :3]\n",
    "print(\"jc_image shape : \", jc_image.shape)\n",
    "print(\"Label Prediction with JC Photo: \", tflearn_improved_model.predict(jc_image))\n",
    "print(\"True Label Value : [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n==============> WITH a non-face image\")\n",
    "# Prediction on unsuitable image (several faces, or animals)\n",
    "if int(PyVersion[0])==2:\n",
    "    data_dir_other = 'Fish.JPEG'\n",
    "elif int(PyVersion[0])==3:\n",
    "    data_dir_other = 'Fish.JPEG'\n",
    "\n",
    "# Reshape for the plot function \n",
    "other_image = plt.imread(data_dir_other)\n",
    "other_image = imresize(other_image, (227, 227))\n",
    "images_to_plot.append(other_image)\n",
    "\n",
    "# Reshape for the prediction model\n",
    "other_image = other_image[np.newaxis]\n",
    "print(\"other_image shape : \", other_image.shape)\n",
    "print(\"Label Prediction with another image: \", tflearn_improved_model.predict(other_image))\n",
    "print(\"True Label Value : Here we have a picture of fishes\")\n",
    "\n",
    "print(\"len images_to_plot :\", len(images_to_plot))\n",
    "\n",
    "for elt in images_to_plot:\n",
    "    print(\"Images shapes for plot :\")\n",
    "    print(elt.shape) \n",
    "\n",
    "fig, axs = plt.subplots(1, len(images_to_plot), figsize=(20, 4))\n",
    "for i, ax_i in enumerate(axs):\n",
    "    NewTitle = \"Test image number \", str(i+1)\n",
    "    ax_i.imshow(images_to_plot[i])\n",
    "    ax_i.set_title(NewTitle)\n",
    "    ax_i.grid('off')\n",
    "    ax_i.axis('off')\n",
    "    \n",
    "print(\"\\n ************ END Improved Model Prediction with TFLearn **************  \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Improvement_Tensorflow\"></a>\n",
    "### II.4.2- With Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**The improved model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ************ START Build Improved Artificial neural network Model with Tensorflow **************  \\n\")\n",
    "\n",
    "from tensorflow.contrib.keras.python.keras.layers.core import *\n",
    "from tensorflow.contrib.keras.python.keras.models import Sequential\n",
    "from tensorflow.contrib.keras.python.keras.models import Model\n",
    "from tensorflow.contrib.keras.python.keras.regularizers import l2\n",
    "from tensorflow.contrib.keras.python.keras.constraints import max_norm\n",
    "from tensorflow.contrib.keras.python.keras.optimizers import SGD\n",
    "\n",
    "#tf.contrib.keras.constraints.MaxNorm\n",
    "#max_value=4\n",
    "#MaxNorm(max_value=4)\n",
    "#kernel_constraint=max_norm(2.)\n",
    "\n",
    "tf_improved_model = Sequential()\n",
    "\n",
    "# YOU COULD REMOVE THE DROPOUT AND INCREASE THE LAYERS SIZE TO COMPARE\n",
    "\n",
    "tf_improved_model.add(Dense(64, activation='relu', input_shape=(154587,), kernel_constraint=max_norm(3.)))\n",
    "tf_improved_model.add(Dropout(0.8))\n",
    "tf_improved_model.add(Dense(64, activation='relu', kernel_constraint=max_norm(3.)))\n",
    "tf_improved_model.add(Dropout(0.5))\n",
    "tf_improved_model.add(Dense(32, activation='relu', kernel_constraint=max_norm(3.)))\n",
    "tf_improved_model.add(Dropout(0.5))\n",
    "tf_improved_model.add(Dense(32, activation='relu', kernel_constraint=max_norm(3.)))\n",
    "tf_improved_model.add(Dropout(0.5))\n",
    "tf_improved_model.add(Dense(16, activation='relu', kernel_constraint=max_norm(3.)))\n",
    "tf_improved_model.add(Dropout(0.5))\n",
    "tf_improved_model.add(Dense(16, activation='relu', kernel_constraint=max_norm(3.)))\n",
    "tf_improved_model.add(Dropout(0.5))\n",
    "tf_improved_model.add(Dense(10, activation='softmax', kernel_constraint=max_norm(3.)))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-7, momentum=.99)\n",
    "tf_improved_model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Reshape it to map the content to the rest of the process\n",
    "X_train = np.array(X_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 154587))\n",
    "y_train = np.array(y_train)\n",
    "print(y_train.shape)\n",
    "y_train_new = y_train.copy()\n",
    "print(y_train_new.shape)\n",
    "\n",
    "# starts training\n",
    "history = tf_improved_model.fit(X_train, y_train_new, validation_split=0.3, epochs=3000, batch_size=128, verbose=0)  \n",
    "\n",
    "\n",
    "print(\"\\n ************ END Build Improved Artificial neural network Model with Tensorflow **************  \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# USEFUL INFORMATION FOR TUNING\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# model.add(Dropout(0.5))\n",
    "# tf.contrib.keras.layers.add(inputs, **kwargs)\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# Dropout: A Simple Way to Prevent Neural Networks from Overfitting\n",
    "# http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\n",
    "# keras.layers.core.Dropout(rate, noise_shape=None, seed=None)\n",
    "\n",
    "\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# keras.layers.core.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', \n",
    "#                        kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, \n",
    "#                        bias_constraint=None)\n",
    "\n",
    "\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# compile(self, optimizer, loss, metrics=None, loss_weights=None, sample_weight_mode=None)\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#fit(self, x=None, y=None, batch_size=32, epochs=1, verbose=1, callbacks=None, validation_split=0.0, \n",
    "#    validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### MODEL BACKUP\n",
    "# If the /output directory doesn't exist you will need to create it.\n",
    "filepath = 'output/improved.model.tensorflow.onehot'\n",
    "tf.contrib.keras.models.save_model(tf_improved_model, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 154587))\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "score = tf_improved_model.evaluate(X_test, y_test, batch_size=128)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "# http://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n ************ START Improved Model Prediction  with Tensorflow **************  \\n\")\n",
    "\n",
    "### With images from the dataset\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "Label = tf_improved_model.predict(np.reshape(X_test[0], (1, 154587)))\n",
    "\n",
    "print(\"Label Prediction : \", Label)\n",
    "print(\"True Label Value : \", y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### With other face images\n",
    "if int(PyVersion[0])==2:\n",
    "    data_dir_jc_photo = 'Kouassi-Jean-Claude-ID.png'\n",
    "elif int(PyVersion[0])==3:\n",
    "    data_dir_jc_photo = 'Kouassi-Jean-Claude-ID.png'\n",
    "\n",
    "# Reshape for the plot function \n",
    "jc_image = plt.imread(data_dir_jc_photo)\n",
    "jc_image = imresize(jc_image, (227, 227))\n",
    "jc_image = jc_image[np.newaxis]\n",
    "\n",
    "print(\"Before reshape : \", jc_image.shape)\n",
    "# So let's remove the Alpha channel on the blended image\n",
    "# https://github.com/davisking/dlib/issues/128\n",
    "jc_image = jc_image[:, :, :, :3]\n",
    "print(\"After reshape : \", jc_image.shape)\n",
    "score = tf_improved_model.predict(np.reshape(jc_image, (1, 154587)))\n",
    "print(\"Prediction score : \", score)\n",
    "print(\"True Label Value : [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### With non-faces images\n",
    "if int(PyVersion[0])==2:\n",
    "    data_dir_other = 'Fish.JPEG'\n",
    "elif int(PyVersion[0])==3:\n",
    "    data_dir_other = 'Fish.JPEG'\n",
    "\n",
    "# Reshape for the plot function \n",
    "other_image = plt.imread(data_dir_other)\n",
    "other_image = imresize(other_image, (227, 227))\n",
    "\n",
    "# Reshape for the prediction model\n",
    "other_image = other_image[np.newaxis]\n",
    "\n",
    "print(other_image.shape)\n",
    "\n",
    "score = tf_improved_model.predict(np.reshape(other_image, (1, 154587)))\n",
    "print(\"Prediction score : \", score)\n",
    "print(\"True Label Value : Here we have a picture of fishes\")\n",
    "\n",
    "print(\"\\n\\n ************ END Improved Model Prediction with Tensorflow **************  \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REUSE SAVED MODELS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Before the fit to train on, or use alone and start to predict on.\n",
    "### 1- TFLEARN\n",
    "# tflearn_improved_model = tflearn_improved_model.load('/output/improved.model.tflearn')\n",
    "\n",
    "### 2- TENSORFLOW\n",
    "#filepath = 'output/improved.model.tensorflow'\n",
    "#tf_improved_model = tf.contrib.keras.models.load_model(filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Age_Gender_Application\"></a>\n",
    "# III- Age and Gender Application (Optional)\n",
    "      - Resources for Desktop, Mobile and Web Application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***Several possibilities to run the App on videos:***\n",
    "- Using the [MoviePy](http://zulko.github.io/moviepy/index.html) Python module for video editing. It works on Windows, Mac, and Linux, with Python 2 or Python 3.\n",
    "- [Python Live Video Streaming Example](http://www.chioka.in/python-live-video-streaming-example/)\n",
    "- Using [scikit-video](https://github.com/aizvorski/scikit-video), a Video processing algorithms, including I/O, quality metrics, temporal filtering, motion/object detection, motion estimation...\n",
    "- Using the [Continuous online video classification with TensorFlow](https://medium.com/@harvitronix/continuous-online-video-classification-with-tensorflow-inception-and-a-raspberry-pi-785c8b1e13e1) post on medium. The [Part2](https://medium.com/@harvitronix/continuous-video-classification-with-tensorflow-inception-and-recurrent-nets-250ba9ff6b85) includes examples on how to reuse TFLearn saved models.\n",
    "- Applying the acgGAN function to **openCV** functions as in the [Udacity-Self-Driving-Car-Preview](https://github.com/Kjeanclaude/Udacity-SDC-Preview); look at the video part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***Now on Mobile:***\n",
    "<font color='red'><br/>There is not enough documentation for this task as I explained in the introduction. <br/>But we could find for Tensorflow some recents posts it is worth to try (almost all of them posted in March 2017).<br/>**Other methods are welcome, mostly universal methods (crossplatform supporting several Deep Learning frameworks).**</font>\n",
    "\n",
    "- [Deploying a TensorFlow model to Android](https://chatbotslife.com/deploying-a-tensorflow-model-to-android-69d04d1b0cba)\n",
    "- [Creating Custom Model For Android Using TensorFlow](https://blog.mindorks.com/creating-custom-model-for-android-using-tensorflow-3f963d270bfb)\n",
    "- Tensorflow codelabs:\n",
    "  - [TensorFlow for Poets 1](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/index.html#0)\n",
    "  - [TensorFlow for Poets 2: Optimize for Mobile](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2/#0)\n",
    "  \n",
    "\n",
    "***Below the intending functionalities of the App:***\n",
    "- Select a picture or take a video with your phone\n",
    "- The App give the gender and age of the picture (For instance, \"It seems that we have a 30 years old Woman in this picture\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Discussion\"></a>\n",
    "# IV- Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement\n",
    "We will try to increase the quality of images as we can (It is the challenge the original paper tried to overcome).\n",
    "We should try to provide result closer to below (even if we could not have the same accuracy because of amount of data and class of ages available with platform such as Microsoft, Facebook, etc.).\n",
    "- For the age and gender prediction on unseen images : https://how-old.net/ (Microsoft). It is very accurate.    \n",
    "    \n",
    "    \n",
    "### Framework limitation\n",
    "Tools we used to train and evaluate the dataset could have an influence on the result. For example, the use of a DNNClassifier or a Keras on Tensorflow has different requirements. The dataset structure should be a dictionary for one, and a list for another. So it would be interesting to evaluate all the possibilities and compare how accurate improvement there is for each use case.\n",
    "\n",
    "### Future work\n",
    "\n",
    "- it would be worth to apply the same technic on other types of ANN such as GAN, as it is a recent field always in exploration, ***mainly the Wasserstein GAN*** which produces accurate results.\n",
    "- propose an ***universal and easy method*** for deployment of notebook implementations on a Mobile App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "# CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We are thankful to Udacity for this effective Machine Learning Engineer Program. We have learned a lot of things and ready to go and apply them to professionnal and personal objectives projects.\n",
    "\n",
    "We also thank the authors of the research paper which inspired our work, [Age and gender classification using convolutional neural networks](http://www.openu.ac.il/home/hassner/projects/cnn_agegender/CNN_AgeGenderEstimation.pdf). Even if they have different objectives, the provided dataset have been very helpful to perform this work. While they used CNN on separate prediction of gender and age, we used FCNs for a pixel-wise precision for both age and gender prediction on the same image, in a unique process.\n",
    "\n",
    "This work could be used as a tutorial for anyone interesting in this field.\n",
    "\n",
    "The main contribution had been:\n",
    "- Provide simple and easy to use tools for dataset preprocessing, **considering different Machine Learning Frameworks requirements**.\n",
    "- Assemble the prediction process in one step for both predictions, age and gender.\n",
    "- Turning the implementation in video and mobile application\n",
    "- Models' performances : 96% accuracy with the tuples model and about 60% of exact accuracy with the 2-hot labels model.\n",
    "\n",
    "This capstone project allowed us to use tools provided by the MLND (Machine Learning Engineer Nanodegree Program) for the Machine Learning Process, especially :\n",
    "    - Supervised Classification\n",
    "    - Data Exploration\n",
    "        --> Feature Observation : Identify and build feature and target columns from the dataset.\n",
    "        Using labeled images (the Adience benchmark, we have been able to recreate a new Unified dataset \n",
    "        of new features and labels to fit our model)\n",
    "    - Performance Metric (Loss and Accuracy Scores)\n",
    "    - Shuffle and Split Data : Training and Testing Data Split (sklearn train_test_split tool)\n",
    "    - Training Models (TFLearn and Tensorflow Keras models)\n",
    "    - Model Evaluation and Validation (Loss and Accuracy)\n",
    "    - Analyzing Model Performance (Tensorboard and Training history Visualization)\n",
    "        --> Learning Curves\n",
    "        --> Complexity Curves\n",
    "    - Making Predictions (of correct labels)\n",
    "    - Model Optimization - Model Tuning (optimizer, activation, loss and regularization functions, number of epoch, etc. )\n",
    "    - Training computational cost (Big-O complexities of common algorithms used in Computer Science)\n",
    "\n",
    "\n",
    "One of the Udacity statement is about the lifelong learning; so after this MLND, as we are involved in Cognitive Computing Research, it will be a solid basis to continue training in the field and reach the goal of Senior Cognitive Computing Researcher.\n",
    "\n",
    "<font color='red'><br/>So, we are pleased to share this work with you. As it is said that there is no perfect human work, we are open to comment or suggestion. <br/>Any positive or negative feedback are welcome.<br/>Thank you !. <br/>==> <a href=\"https://github.com/Kjeanclaude\">K. Jean-Claude</a><br/></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color='green'><br/>“Intellectuals solve problems, geniuses prevent them.” <br/>“The difference between stupidity and genius is that genius has its limits.”<br/><a href=\"http://www.movemequotes.com/top-21-albert-einstein-quotes/\">Albert-Einstein</a><br/></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
